{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "06e5dfb7",
   "metadata": {},
   "source": [
    "# Portfolio Analyzer for Vision-Capable LLMs\n",
    "\n",
    "This notebook demonstrates the comprehensive **Analyzer** class that generates metrics, Greeks, and visualizations specifically designed for Vision-Capable LLM analysis.\n",
    "\n",
    "## ğŸ¯ What the Analyzer Does\n",
    "\n",
    "The `Analyzer` class takes a `Portfolio` object and generates:\n",
    "1. **Comprehensive Metrics** - Performance, risk, allocation, and time-based analysis\n",
    "2. **Portfolio Greeks** - Delta, Gamma, Theta, Vega, Rho calculations\n",
    "3. **LLM-Ready Visualizations** - Base64 encoded images optimized for vision models\n",
    "\n",
    "All outputs are structured in JSON format for easy LLM consumption."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2500b2a6",
   "metadata": {},
   "source": [
    "## ğŸ“¦ Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab4c428f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import sys\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import json\n",
    "from datetime import datetime, timedelta\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Add the src directory to the path to import our package\n",
    "sys.path.append('../src')\n",
    "\n",
    "# Import our portfolio analytics package\n",
    "from portfolio_analytics import Portfolio, DataProvider, Analyzer\n",
    "\n",
    "print(\"âœ… All imports successful!\")\n",
    "print(f\"ğŸ“… Analysis Date: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdbfb0ad",
   "metadata": {},
   "source": [
    "## ğŸ—ï¸ Create Sample Portfolio\n",
    "\n",
    "Let's create a technology-focused portfolio for demonstration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3df78a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define portfolio components\n",
    "symbols = ['AAPL', 'GOOGL', 'MSFT', 'AMZN', 'TSLA']\n",
    "weights = [0.30, 0.25, 0.20, 0.15, 0.10]  # Strategic allocation\n",
    "portfolio_name = \"Tech Growth Portfolio\"\n",
    "\n",
    "# Create portfolio object\n",
    "portfolio = Portfolio(\n",
    "    symbols=symbols, \n",
    "    weights=weights, \n",
    "    name=portfolio_name\n",
    ")\n",
    "\n",
    "print(f\"ğŸ“Š Created Portfolio: {portfolio_name}\")\n",
    "print(f\"ğŸ¢ Assets: {', '.join(symbols)}\")\n",
    "print(f\"âš–ï¸ Weights: {[f'{w:.1%}' for w in weights]}\")\n",
    "print(f\"âœ… Weights sum to: {sum(weights):.1%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b339d0e",
   "metadata": {},
   "source": [
    "## ğŸ“ˆ Load Portfolio Data\n",
    "\n",
    "For this demonstration, we'll create synthetic data that mimics real market behavior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce206ba4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create synthetic data for demonstration\n",
    "def create_synthetic_portfolio_data(symbols, start_date, end_date, seed=42):\n",
    "    \"\"\"\n",
    "    Create realistic synthetic stock data for demonstration purposes.\n",
    "    \"\"\"\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    # Generate date range\n",
    "    date_range = pd.date_range(start=start_date, end=end_date, freq='D')\n",
    "    \n",
    "    # Remove weekends (simulate trading days only)\n",
    "    trading_days = date_range[date_range.weekday < 5]\n",
    "    \n",
    "    # Synthetic parameters for each stock\n",
    "    stock_params = {\n",
    "        'AAPL': {'initial_price': 150, 'drift': 0.08, 'volatility': 0.25},\n",
    "        'GOOGL': {'initial_price': 2800, 'drift': 0.10, 'volatility': 0.28},\n",
    "        'MSFT': {'initial_price': 300, 'drift': 0.09, 'volatility': 0.24},\n",
    "        'AMZN': {'initial_price': 3300, 'drift': 0.12, 'volatility': 0.32},\n",
    "        'TSLA': {'initial_price': 800, 'drift': 0.15, 'volatility': 0.45}\n",
    "    }\n",
    "    \n",
    "    # Generate correlated price data\n",
    "    n_days = len(trading_days)\n",
    "    dt = 1/252  # Daily time step\n",
    "    \n",
    "    # Create correlation matrix (tech stocks are correlated)\n",
    "    correlation_matrix = np.array([\n",
    "        [1.00, 0.70, 0.75, 0.65, 0.50],  # AAPL\n",
    "        [0.70, 1.00, 0.68, 0.72, 0.45],  # GOOGL\n",
    "        [0.75, 0.68, 1.00, 0.70, 0.40],  # MSFT\n",
    "        [0.65, 0.72, 0.70, 1.00, 0.55],  # AMZN\n",
    "        [0.50, 0.45, 0.40, 0.55, 1.00]   # TSLA\n",
    "    ])\n",
    "    \n",
    "    # Generate correlated random shocks\n",
    "    independent_shocks = np.random.normal(0, 1, (n_days, len(symbols)))\n",
    "    \n",
    "    # Apply correlation using Cholesky decomposition\n",
    "    L = np.linalg.cholesky(correlation_matrix)\n",
    "    correlated_shocks = independent_shocks @ L.T\n",
    "    \n",
    "    # Generate price paths\n",
    "    price_data = {}\n",
    "    \n",
    "    for i, symbol in enumerate(symbols):\n",
    "        params = stock_params[symbol]\n",
    "        \n",
    "        # Geometric Brownian Motion\n",
    "        returns = (params['drift'] - 0.5 * params['volatility']**2) * dt + \\\n",
    "                 params['volatility'] * np.sqrt(dt) * correlated_shocks[:, i]\n",
    "        \n",
    "        # Add some market regime changes and volatility clustering\n",
    "        # Market crash simulation (rare events)\n",
    "        crash_prob = 0.002  # 0.2% daily probability\n",
    "        crash_events = np.random.random(n_days) < crash_prob\n",
    "        returns[crash_events] += np.random.normal(-0.05, 0.02, crash_events.sum())\n",
    "        \n",
    "        # Volatility clustering (GARCH-like)\n",
    "        for j in range(1, len(returns)):\n",
    "            if abs(returns[j-1]) > 0.03:  # Previous day was volatile\n",
    "                returns[j] *= 1.5  # Increase today's volatility\n",
    "        \n",
    "        # Calculate cumulative prices\n",
    "        prices = params['initial_price'] * np.exp(np.cumsum(returns))\n",
    "        price_data[symbol] = prices\n",
    "    \n",
    "    # Create DataFrame\n",
    "    df = pd.DataFrame(price_data, index=trading_days)\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Generate synthetic data\n",
    "start_date = \"2022-01-01\"\n",
    "end_date = \"2024-07-01\"\n",
    "\n",
    "print(\"ğŸ”„ Generating synthetic market data...\")\n",
    "synthetic_data = create_synthetic_portfolio_data(symbols, start_date, end_date)\n",
    "\n",
    "# Manually set the data and calculate returns (since we're using synthetic data)\n",
    "portfolio.data = synthetic_data\n",
    "portfolio.returns = portfolio.data.pct_change().dropna()\n",
    "\n",
    "print(f\"ğŸ“Š Generated {len(portfolio.data)} trading days of data\")\n",
    "print(f\"ğŸ“… Period: {portfolio.data.index.min().date()} to {portfolio.data.index.max().date()}\")\n",
    "print(f\"ğŸ’¹ Price ranges:\")\n",
    "for symbol in symbols:\n",
    "    min_price = portfolio.data[symbol].min()\n",
    "    max_price = portfolio.data[symbol].max()\n",
    "    final_price = portfolio.data[symbol].iloc[-1]\n",
    "    print(f\"  {symbol}: ${min_price:.0f} - ${max_price:.0f} (Final: ${final_price:.0f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "626f7fb9",
   "metadata": {},
   "source": [
    "## ğŸ” Initialize the Analyzer\n",
    "\n",
    "Now let's create the Analyzer object and see what it can do."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b977e32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the Analyzer\n",
    "analyzer = Analyzer(portfolio)\n",
    "\n",
    "print(\"ğŸ”¬ Portfolio Analyzer initialized!\")\n",
    "print(f\"ğŸ“Š Analyzing portfolio: {portfolio.name}\")\n",
    "print(f\"ğŸ¢ Assets under analysis: {len(portfolio.symbols)}\")\n",
    "print(f\"ğŸ“ˆ Data points: {len(portfolio.returns)} daily returns\")\n",
    "\n",
    "# Show the analyzer's capabilities\n",
    "print(\"\\nğŸ› ï¸ Analyzer Capabilities:\")\n",
    "capabilities = [\n",
    "    \"âœ… Performance Metrics (Returns, Sharpe, Sortino, etc.)\",\n",
    "    \"âœ… Risk Metrics (VaR, Expected Shortfall, Drawdowns)\", \n",
    "    \"âœ… Portfolio Greeks (Delta, Gamma, Theta, Vega, Rho)\",\n",
    "    \"âœ… Asset-level Analysis\",\n",
    "    \"âœ… Time-based Performance Analysis\",\n",
    "    \"âœ… Stress Testing & Sensitivity Analysis\",\n",
    "    \"âœ… 10+ Visualization Charts (Base64 Encoded)\",\n",
    "    \"âœ… LLM-optimized JSON Output\"\n",
    "]\n",
    "\n",
    "for capability in capabilities:\n",
    "    print(f\"  {capability}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33ecd89a",
   "metadata": {},
   "source": [
    "## ğŸ“Š Generate Comprehensive Analysis\n",
    "\n",
    "Let's run the full analysis and see all the metrics and Greeks generated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9cacbe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate comprehensive analysis\n",
    "print(\"ğŸ”„ Running comprehensive portfolio analysis...\")\n",
    "print(\"This includes: Metrics + Greeks + Visualizations\")\n",
    "\n",
    "# Run the analysis\n",
    "analysis_results = analyzer.export_for_llm(output_format=\"comprehensive\")\n",
    "\n",
    "print(\"\\nâœ… Analysis Complete!\")\n",
    "print(f\"ğŸ“¦ Generated {len(analysis_results)} main sections:\")\n",
    "for section_name in analysis_results.keys():\n",
    "    print(f\"  ğŸ“‹ {section_name}\")\n",
    "\n",
    "# Show the structure\n",
    "print(\"\\nğŸ“Š Metrics Categories:\")\n",
    "for category in analysis_results['metrics'].keys():\n",
    "    count = len(analysis_results['metrics'][category])\n",
    "    print(f\"  â€¢ {category}: {count} metrics\")\n",
    "\n",
    "print(\"\\nğŸ”¢ Greeks Categories:\")\n",
    "for category in analysis_results['greeks'].keys():\n",
    "    if isinstance(analysis_results['greeks'][category], dict):\n",
    "        count = len(analysis_results['greeks'][category])\n",
    "        print(f\"  â€¢ {category}: {count} calculations\")\n",
    "\n",
    "print(\"\\nğŸ“ˆ Visualizations Generated:\")\n",
    "for viz_name in analysis_results['visualizations'].keys():\n",
    "    print(f\"  ğŸ–¼ï¸ {viz_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1e391ab",
   "metadata": {},
   "source": [
    "## ğŸ“ˆ Portfolio Performance Summary\n",
    "\n",
    "Let's examine the key performance metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9432c42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract key metrics for display\n",
    "portfolio_summary = analysis_results['portfolio_summary']\n",
    "perf_metrics = analysis_results['metrics']['performance']\n",
    "risk_metrics = analysis_results['metrics']['risk']\n",
    "allocation_metrics = analysis_results['metrics']['allocation']\n",
    "\n",
    "print(\"ğŸ“Š PORTFOLIO PERFORMANCE SUMMARY\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "print(f\"\\nğŸ¢ Portfolio: {portfolio_summary['portfolio_name']}\")\n",
    "print(f\"ğŸ“… Analysis Period: {portfolio_summary['data_start_date']} to {portfolio_summary['data_end_date']}\")\n",
    "print(f\"ğŸ“Š Total Observations: {portfolio_summary['total_observations']:,} trading days\")\n",
    "\n",
    "print(\"\\nğŸ’° RETURN METRICS:\")\n",
    "print(f\"  ğŸ“ˆ Total Return: {perf_metrics.get('total_return', 0):.2%}\")\n",
    "print(f\"  ğŸ“Š Annual Return: {perf_metrics.get('annual_return', 0):.2%}\")\n",
    "print(f\"  ğŸ“Š Average Monthly Return: {perf_metrics.get('average_monthly_return', 0):.2%}\")\n",
    "print(f\"  ğŸš€ Best Month: {perf_metrics.get('best_month', 0):.2%}\")\n",
    "print(f\"  ğŸ“‰ Worst Month: {perf_metrics.get('worst_month', 0):.2%}\")\n",
    "print(f\"  âœ… Positive Months: {perf_metrics.get('positive_months', 0)}\")\n",
    "print(f\"  âŒ Negative Months: {perf_metrics.get('negative_months', 0)}\")\n",
    "\n",
    "print(\"\\nâš ï¸ RISK METRICS:\")\n",
    "print(f\"  ğŸ“Š Annual Volatility: {perf_metrics.get('annual_volatility', 0):.2%}\")\n",
    "print(f\"  ğŸ“‰ Maximum Drawdown: {perf_metrics.get('max_drawdown', 0):.2%}\")\n",
    "print(f\"  ğŸ“Š VaR (95%): {risk_metrics.get('var_95', 0):.2%}\")\n",
    "print(f\"  ğŸ“Š Expected Shortfall (95%): {risk_metrics.get('expected_shortfall_95', 0):.2%}\")\n",
    "print(f\"  ğŸ“Š Skewness: {risk_metrics.get('skewness', 0):.3f}\")\n",
    "print(f\"  ğŸ“Š Kurtosis: {risk_metrics.get('kurtosis', 0):.3f}\")\n",
    "\n",
    "print(\"\\nğŸ¯ RISK-ADJUSTED METRICS:\")\n",
    "print(f\"  â­ Sharpe Ratio: {perf_metrics.get('sharpe_ratio', 0):.3f}\")\n",
    "print(f\"  ğŸ“Š Sortino Ratio: {perf_metrics.get('sortino_ratio', 0):.3f}\")\n",
    "print(f\"  ğŸ“Š Calmar Ratio: {perf_metrics.get('calmar_ratio', 0):.3f}\")\n",
    "\n",
    "print(\"\\nâš–ï¸ PORTFOLIO COMPOSITION:\")\n",
    "for symbol, weight in allocation_metrics['weights'].items():\n",
    "    print(f\"  ğŸ¢ {symbol}: {weight:.1%}\")\n",
    "\n",
    "print(f\"\\nğŸ“Š PORTFOLIO STRUCTURE:\")\n",
    "print(f\"  ğŸ¯ Concentration (HHI): {allocation_metrics['weight_concentration_hhi']:.3f}\")\n",
    "print(f\"  ğŸ“Š Largest Position: {allocation_metrics['largest_position']:.1%}\")\n",
    "print(f\"  ğŸ”¢ Effective # of Assets: {analysis_results['metrics']['portfolio']['effective_number_of_assets']:.1f}\")\n",
    "print(f\"  ğŸ¢ Positions > 5%: {allocation_metrics['number_positions_over_5pct']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dc6d19f",
   "metadata": {},
   "source": [
    "## ğŸ”¢ Portfolio Greeks Analysis\n",
    "\n",
    "Let's examine the portfolio Greeks - these measure various sensitivities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62dd3e4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract Greeks for analysis\n",
    "portfolio_greeks = analysis_results['greeks']['portfolio_greeks']\n",
    "asset_greeks = analysis_results['greeks']['asset_greeks']\n",
    "\n",
    "print(\"ğŸ”¢ PORTFOLIO GREEKS ANALYSIS\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "print(\"\\nğŸ“Š PORTFOLIO-LEVEL GREEKS:\")\n",
    "print(f\"  ğŸ“ˆ Delta (Market Sensitivity): {portfolio_greeks.get('portfolio_delta', 0):.3f}\")\n",
    "print(f\"  ğŸ”„ Gamma (Convexity): {portfolio_greeks.get('portfolio_gamma', 0):.3f}\")\n",
    "print(f\"  â° Theta (Time Decay): {portfolio_greeks.get('portfolio_theta', 0):.3f}\")\n",
    "print(f\"  ğŸŒŠ Vega (Vol Sensitivity): {portfolio_greeks.get('portfolio_vega', 0):.3f}\")\n",
    "print(f\"  ğŸ’° Rho (Interest Rate Sensitivity): {portfolio_greeks.get('portfolio_rho', 0):.3f}\")\n",
    "\n",
    "print(\"\\nğŸ¢ ASSET-LEVEL ANALYSIS:\")\n",
    "print(\"Asset      | Weight | Delta  | Beta   | Alpha  | Vol    | Risk Contrib\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "for symbol in symbols:\n",
    "    asset_data = asset_greeks[symbol]\n",
    "    print(f\"{symbol:10} | {asset_data['weight']:5.1%} | {asset_data['delta']:6.3f} | \"\n",
    "          f\"{asset_data['beta']:6.3f} | {asset_data['alpha']:6.3f} | \"\n",
    "          f\"{asset_data['volatility']:5.1%} | {asset_data['contribution_to_risk']:11.4f}\")\n",
    "\n",
    "print(\"\\nğŸ“Š INTERPRETATION:\")\n",
    "interpretations = [\n",
    "    f\"â€¢ Delta {portfolio_greeks.get('portfolio_delta', 0):.3f}: Portfolio correlation with market\",\n",
    "    f\"â€¢ Gamma {portfolio_greeks.get('portfolio_gamma', 0):.3f}: Rate of change of delta (convexity)\",\n",
    "    f\"â€¢ Theta {portfolio_greeks.get('portfolio_theta', 0):.3f}: Time decay effect on returns\",\n",
    "    f\"â€¢ Vega {portfolio_greeks.get('portfolio_vega', 0):.3f}: Sensitivity to volatility changes\",\n",
    "    f\"â€¢ Rho {portfolio_greeks.get('portfolio_rho', 0):.3f}: Interest rate sensitivity\"\n",
    "]\n",
    "\n",
    "for interpretation in interpretations:\n",
    "    print(interpretation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb0b3ec7",
   "metadata": {},
   "source": [
    "## ğŸ“Š Time-Based Performance Analysis\n",
    "\n",
    "Let's look at how the portfolio performed across different time periods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6110f9d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract time-based metrics\n",
    "time_metrics = analysis_results['metrics']['time_based']\n",
    "\n",
    "print(\"ğŸ“… TIME-BASED PERFORMANCE ANALYSIS\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Monthly statistics\n",
    "monthly_stats = time_metrics['monthly_stats']\n",
    "print(\"\\nğŸ“Š MONTHLY PERFORMANCE:\")\n",
    "print(f\"  ğŸ“ˆ Average Monthly Return: {monthly_stats['mean']:.2%}\")\n",
    "print(f\"  ğŸ“Š Monthly Volatility: {monthly_stats['std']:.2%}\")\n",
    "print(f\"  ğŸš€ Best Month: {monthly_stats['best_month']:.2%}\")\n",
    "print(f\"  ğŸ“‰ Worst Month: {monthly_stats['worst_month']:.2%}\")\n",
    "print(f\"  âœ… Positive Months: {monthly_stats['positive_months_pct']:.1%}\")\n",
    "print(f\"  ğŸ“Š Skewness: {monthly_stats['skew']:.3f}\")\n",
    "print(f\"  ğŸ“Š Kurtosis: {monthly_stats['kurtosis']:.3f}\")\n",
    "\n",
    "# Quarterly statistics\n",
    "quarterly_stats = time_metrics['quarterly_stats']\n",
    "print(\"\\nğŸ“Š QUARTERLY PERFORMANCE:\")\n",
    "print(f\"  ğŸ“ˆ Average Quarterly Return: {quarterly_stats['mean']:.2%}\")\n",
    "print(f\"  ğŸ“Š Quarterly Volatility: {quarterly_stats['std']:.2%}\")\n",
    "print(f\"  ğŸš€ Best Quarter: {quarterly_stats['best_quarter']:.2%}\")\n",
    "print(f\"  ğŸ“‰ Worst Quarter: {quarterly_stats['worst_quarter']:.2%}\")\n",
    "\n",
    "# Yearly statistics\n",
    "yearly_stats = time_metrics['yearly_stats']\n",
    "print(\"\\nğŸ“Š YEARLY PERFORMANCE:\")\n",
    "print(f\"  ğŸ“ˆ Average Yearly Return: {yearly_stats['mean']:.2%}\")\n",
    "print(f\"  ğŸ“Š Yearly Volatility: {yearly_stats['std']:.2%}\")\n",
    "print(f\"  âœ… Positive Years: {yearly_stats['positive_years_pct']:.1%}\")\n",
    "\n",
    "# Calculate some additional insights\n",
    "win_rate_monthly = monthly_stats['positive_months_pct']\n",
    "avg_win = monthly_stats['best_month'] if monthly_stats['best_month'] > 0 else 0\n",
    "avg_loss = abs(monthly_stats['worst_month']) if monthly_stats['worst_month'] < 0 else 0\n",
    "\n",
    "print(\"\\nğŸ¯ PERFORMANCE INSIGHTS:\")\n",
    "print(f\"  ğŸ“Š Monthly Win Rate: {win_rate_monthly:.1%}\")\n",
    "print(f\"  ğŸ“ˆ Risk-Reward Ratio: {avg_win/avg_loss:.2f}x\" if avg_loss > 0 else \"  ğŸ“ˆ Risk-Reward Ratio: N/A\")\n",
    "print(f\"  ğŸ“Š Return Consistency: {'High' if monthly_stats['std'] < 0.05 else 'Moderate' if monthly_stats['std'] < 0.10 else 'Low'}\")\n",
    "print(f\"  ğŸ“Š Distribution Shape: {'Normal' if abs(monthly_stats['skew']) < 0.5 else 'Skewed'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b99741db",
   "metadata": {},
   "source": [
    "## ğŸ“ˆ Visualization Showcase\n",
    "\n",
    "The Analyzer generates multiple charts as base64 encoded images. Let's see what's available and display some key charts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86db84fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show available visualizations\n",
    "visualizations = analysis_results['visualizations']\n",
    "\n",
    "print(\"ğŸ“ˆ VISUALIZATION CAPABILITIES\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "print(f\"\\nğŸ–¼ï¸ Generated {len(visualizations)} charts for LLM analysis:\")\n",
    "for i, (viz_name, viz_data) in enumerate(visualizations.items(), 1):\n",
    "    # Get the size of the base64 encoded image\n",
    "    size_kb = len(viz_data) * 3 / 4 / 1024  # Approximate size in KB\n",
    "    print(f\"  {i:2d}. {viz_name:25} ({size_kb:.1f} KB)\")\n",
    "\n",
    "print(\"\\nğŸ¯ CHART DESCRIPTIONS:\")\n",
    "descriptions = {\n",
    "    'price_history': 'ğŸ“Š Historical price movements of all assets',\n",
    "    'returns_distribution': 'ğŸ“ˆ Distribution analysis with Q-Q plots and volatility',\n",
    "    'correlation_matrix': 'ğŸ”— Asset correlation heatmap',\n",
    "    'portfolio_composition': 'ğŸ¥§ Portfolio weights (pie & bar charts)',\n",
    "    'cumulative_returns': 'ğŸ“ˆ Portfolio performance over time',\n",
    "    'drawdown_analysis': 'ğŸ“‰ Drawdown periods and recovery analysis',\n",
    "    'risk_return_scatter': 'ğŸ¯ Risk-return positioning of assets',\n",
    "    'rolling_metrics': 'ğŸ“Š Rolling performance metrics over time',\n",
    "    'performance_heatmap': 'ğŸŒ¡ï¸ Monthly performance calendar',\n",
    "    'greek_sensitivity': 'ğŸ”¢ Greeks and sensitivity analysis'\n",
    "}\n",
    "\n",
    "for viz_name, description in descriptions.items():\n",
    "    if viz_name in visualizations:\n",
    "        print(f\"  â€¢ {description}\")\n",
    "\n",
    "print(\"\\nâœ¨ All charts are generated as high-resolution, base64-encoded PNG images\")\n",
    "print(\"ğŸ“¤ Ready for direct input to Vision-Capable LLMs\")\n",
    "print(f\"ğŸ’¾ Total visualization data: {sum(len(v) for v in visualizations.values()) / 1024 / 1024:.1f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "191f531b",
   "metadata": {},
   "source": [
    "## ğŸ›ï¸ Different Export Formats\n",
    "\n",
    "The Analyzer supports multiple export formats for different use cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7f06f01",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ğŸ›ï¸ EXPORT FORMAT DEMONSTRATION\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Test different export formats\n",
    "formats = ['comprehensive', 'summary', 'metrics_only', 'visuals_only']\n",
    "\n",
    "for format_type in formats:\n",
    "    print(f\"\\nğŸ“¦ Testing '{format_type}' format...\")\n",
    "    export_data = analyzer.export_for_llm(output_format=format_type)\n",
    "    \n",
    "    # Analyze the export\n",
    "    sections = list(export_data.keys())\n",
    "    total_size = len(str(export_data))\n",
    "    \n",
    "    print(f\"  ğŸ“‹ Sections: {', '.join(sections)}\")\n",
    "    print(f\"  ğŸ“Š Data size: {total_size:,} characters\")\n",
    "    \n",
    "    # Format-specific analysis\n",
    "    if format_type == 'comprehensive':\n",
    "        metrics_count = sum(len(cat) for cat in export_data.get('metrics', {}).values() if isinstance(cat, dict))\n",
    "        greeks_count = sum(len(cat) for cat in export_data.get('greeks', {}).values() if isinstance(cat, dict))\n",
    "        viz_count = len(export_data.get('visualizations', {}))\n",
    "        print(f\"  ğŸ“Š Metrics: {metrics_count}, Greeks: {greeks_count}, Charts: {viz_count}\")\n",
    "        \n",
    "    elif format_type == 'summary':\n",
    "        key_metrics = export_data.get('key_metrics', {})\n",
    "        viz_count = len(export_data.get('visualizations', {}))\n",
    "        print(f\"  ğŸ“Š Key metrics: {len(key_metrics)}, Charts: {viz_count}\")\n",
    "        \n",
    "    elif format_type == 'metrics_only':\n",
    "        total_metrics = sum(len(cat) for cat in export_data.values() if isinstance(cat, dict))\n",
    "        print(f\"  ğŸ“Š Total metrics: {total_metrics}\")\n",
    "        \n",
    "    elif format_type == 'visuals_only':\n",
    "        viz_count = len(export_data)\n",
    "        avg_size = sum(len(v) for v in export_data.values()) / len(export_data) / 1024\n",
    "        print(f\"  ğŸ–¼ï¸ Charts: {viz_count}, Avg size: {avg_size:.1f} KB\")\n",
    "\n",
    "print(\"\\nğŸ¯ USE CASE RECOMMENDATIONS:\")\n",
    "recommendations = [\n",
    "    \"ğŸ“Š 'comprehensive' â†’ Full portfolio analysis and reporting\",\n",
    "    \"ğŸ“‹ 'summary' â†’ Quick portfolio overview with key visuals\", \n",
    "    \"ğŸ”¢ 'metrics_only' â†’ Quantitative analysis without charts\",\n",
    "    \"ğŸ–¼ï¸ 'visuals_only' â†’ Chart analysis and pattern recognition\"\n",
    "]\n",
    "\n",
    "for rec in recommendations:\n",
    "    print(f\"  {rec}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41a9693a",
   "metadata": {},
   "source": [
    "## ğŸ§ª Stress Testing & Sensitivity Analysis\n",
    "\n",
    "The Analyzer includes stress testing capabilities to understand portfolio behavior under different scenarios."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "763d16aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract stress testing and sensitivity analysis\n",
    "sensitivity_analysis = analysis_results['greeks']['sensitivity_analysis']\n",
    "stress_scenarios = sensitivity_analysis['stress_scenarios']\n",
    "\n",
    "print(\"ğŸ§ª STRESS TESTING & SENSITIVITY ANALYSIS\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "print(\"\\nğŸ“‰ MARKET STRESS SCENARIOS:\")\n",
    "\n",
    "# Market crash scenarios\n",
    "crash_10 = stress_scenarios['market_crash_10pct']\n",
    "crash_20 = stress_scenarios['market_crash_20pct']\n",
    "\n",
    "print(\"\\nğŸ”¥ Market Crash -10%:\")\n",
    "print(f\"  ğŸ“Š Scenario Return: {crash_10['scenario_return']:.2%}\")\n",
    "print(f\"  ğŸ“Š Scenario Volatility: {crash_10['scenario_volatility']:.2%}\")\n",
    "print(f\"  ğŸ“Š VaR (95%): {crash_10['var_95']:.2%}\")\n",
    "print(f\"  ğŸ“Š Expected Shortfall: {crash_10['expected_shortfall']:.2%}\")\n",
    "\n",
    "print(\"\\nğŸ’¥ Market Crash -20%:\")\n",
    "print(f\"  ğŸ“Š Scenario Return: {crash_20['scenario_return']:.2%}\")\n",
    "print(f\"  ğŸ“Š Scenario Volatility: {crash_20['scenario_volatility']:.2%}\")\n",
    "print(f\"  ğŸ“Š VaR (95%): {crash_20['var_95']:.2%}\")\n",
    "print(f\"  ğŸ“Š Expected Shortfall: {crash_20['expected_shortfall']:.2%}\")\n",
    "\n",
    "# Volatility stress test\n",
    "vol_stress = stress_scenarios['volatility_spike_50pct']\n",
    "print(\"\\nğŸŒŠ Volatility Spike +50%:\")\n",
    "print(f\"  ğŸ“Š Base Volatility: {vol_stress['base_volatility']:.2%}\")\n",
    "print(f\"  ğŸ“Š Stressed Volatility: {vol_stress['stressed_volatility']:.2%}\")\n",
    "print(f\"  ğŸ“Š Volatility Impact: {vol_stress['volatility_impact']:.2%}\")\n",
    "\n",
    "# Correlation analysis\n",
    "corr_stress = stress_scenarios['correlation_spike']\n",
    "print(\"\\nğŸ”— CORRELATION ANALYSIS:\")\n",
    "print(f\"  ğŸ“Š Average Correlation: {corr_stress['average_correlation']:.3f}\")\n",
    "print(f\"  ğŸ“Š Maximum Correlation: {corr_stress['max_correlation']:.3f}\")\n",
    "print(f\"  ğŸ“Š Minimum Correlation: {corr_stress['min_correlation']:.3f}\")\n",
    "\n",
    "# Factor loadings\n",
    "factor_loadings = sensitivity_analysis['factor_loadings']\n",
    "print(\"\\nğŸ“Š FACTOR ANALYSIS:\")\n",
    "print(f\"  ğŸ“ˆ Market Loading: {factor_loadings['market_loading']:.3f}\")\n",
    "print(f\"  ğŸ“Š R-Squared: {factor_loadings['r_squared']:.3f}\")\n",
    "\n",
    "# Regime analysis\n",
    "regime_analysis = sensitivity_analysis['regime_analysis']\n",
    "print(\"\\nğŸ”„ MARKET REGIME ANALYSIS:\")\n",
    "\n",
    "high_vol = regime_analysis['high_volatility_regime']\n",
    "low_vol = regime_analysis['low_volatility_regime']\n",
    "\n",
    "print(f\"\\nğŸ“ˆ High Volatility Regime:\")\n",
    "print(f\"  ğŸ“Š Periods: {high_vol['periods']} days\")\n",
    "print(f\"  ğŸ“Š Average Return: {high_vol['avg_return']:.2%}\")\n",
    "print(f\"  ğŸ“Š Average Volatility: {high_vol['avg_volatility']:.2%}\")\n",
    "\n",
    "print(f\"\\nğŸ“‰ Low Volatility Regime:\")\n",
    "print(f\"  ğŸ“Š Periods: {low_vol['periods']} days\")\n",
    "print(f\"  ğŸ“Š Average Return: {low_vol['avg_return']:.2%}\")\n",
    "print(f\"  ğŸ“Š Average Volatility: {low_vol['avg_volatility']:.2%}\")\n",
    "\n",
    "print(\"\\nğŸ¯ STRESS TEST INSIGHTS:\")\n",
    "insights = [\n",
    "    f\"â€¢ Portfolio shows {abs(crash_10['scenario_return']):.1%} return decline in -10% market stress\",\n",
    "    f\"â€¢ Volatility increases by {vol_stress['volatility_impact']:.1%} under stress conditions\",\n",
    "    f\"â€¢ Average asset correlation is {corr_stress['average_correlation']:.2f} (diversification benefit)\",\n",
    "    f\"â€¢ Market loading of {factor_loadings['market_loading']:.2f} indicates {'high' if abs(factor_loadings['market_loading']) > 0.7 else 'moderate'} market sensitivity\"\n",
    "]\n",
    "\n",
    "for insight in insights:\n",
    "    print(f\"  {insight}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97742efa",
   "metadata": {},
   "source": [
    "## ğŸ’¾ Export for LLM Analysis\n",
    "\n",
    "Finally, let's show how the complete analysis package would be formatted for a Vision-Capable LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89b34dea",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ğŸ’¾ LLM EXPORT PACKAGE SUMMARY\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Create a summary of what would be sent to the LLM\n",
    "llm_package = analyzer.export_for_llm(output_format=\"comprehensive\")\n",
    "\n",
    "# Calculate package statistics\n",
    "def analyze_data_structure(data, prefix=\"\"):\n",
    "    stats = {}\n",
    "    \n",
    "    if isinstance(data, dict):\n",
    "        for key, value in data.items():\n",
    "            current_key = f\"{prefix}.{key}\" if prefix else key\n",
    "            if isinstance(value, dict):\n",
    "                stats[current_key] = f\"Dict with {len(value)} items\"\n",
    "                nested_stats = analyze_data_structure(value, current_key)\n",
    "                stats.update(nested_stats)\n",
    "            elif isinstance(value, list):\n",
    "                stats[current_key] = f\"List with {len(value)} items\"\n",
    "            elif isinstance(value, str) and len(value) > 1000:\n",
    "                stats[current_key] = f\"Base64 image ({len(value)/1024:.1f} KB)\"\n",
    "            else:\n",
    "                stats[current_key] = type(value).__name__\n",
    "    \n",
    "    return stats\n",
    "\n",
    "package_structure = analyze_data_structure(llm_package)\n",
    "\n",
    "print(\"ğŸ“¦ PACKAGE CONTENTS:\")\n",
    "print(f\"  ğŸ“Š Total sections: {len(llm_package)}\")\n",
    "print(f\"  ğŸ’¾ Estimated size: {len(str(llm_package))/1024/1024:.1f} MB\")\n",
    "\n",
    "# Count different types of data\n",
    "metrics_count = 0\n",
    "image_count = 0\n",
    "image_size = 0\n",
    "\n",
    "for key, value in package_structure.items():\n",
    "    if \"Base64 image\" in value:\n",
    "        image_count += 1\n",
    "        # Extract size from the description\n",
    "        size_str = value.split(\"(\")[1].split(\" KB\")[0]\n",
    "        image_size += float(size_str)\n",
    "    elif key.startswith(\"metrics.\"):\n",
    "        metrics_count += 1\n",
    "\n",
    "print(f\"\\nğŸ“Š DATA BREAKDOWN:\")\n",
    "print(f\"  ğŸ”¢ Numerical metrics: {metrics_count}\")\n",
    "print(f\"  ğŸ–¼ï¸ Visualization images: {image_count}\")\n",
    "print(f\"  ğŸ’¾ Total image data: {image_size:.1f} KB\")\n",
    "\n",
    "print(\"\\nğŸ¤– LLM INPUT SPECIFICATION:\")\n",
    "llm_spec = [\n",
    "    \"ğŸ“‹ Structured JSON format for easy parsing\",\n",
    "    \"ğŸ“Š 150+ quantitative metrics and ratios\",\n",
    "    \"ğŸ”¢ Complete Greeks analysis (5 main Greeks)\",\n",
    "    \"ğŸ–¼ï¸ 10 high-resolution charts as base64 PNG\",\n",
    "    \"âš ï¸ Comprehensive risk and stress testing\",\n",
    "    \"ğŸ“… Time-series performance analysis\",\n",
    "    \"ğŸ¯ Portfolio optimization insights\",\n",
    "    \"ğŸ“ˆ Asset allocation recommendations\"\n",
    "]\n",
    "\n",
    "for spec in llm_spec:\n",
    "    print(f\"  {spec}\")\n",
    "\n",
    "print(\"\\nâœ¨ READY FOR VISION-CAPABLE LLM ANALYSIS!\")\n",
    "print(\"\\nğŸ¯ The complete package includes:\")\n",
    "final_summary = [\n",
    "    \"â€¢ All numerical analysis in structured JSON format\",\n",
    "    \"â€¢ Visual charts encoded as base64 images for vision models\",\n",
    "    \"â€¢ Comprehensive metadata and timestamps\",\n",
    "    \"â€¢ Multiple export formats for different use cases\",\n",
    "    \"â€¢ Professional-grade financial metrics and Greeks\",\n",
    "    \"â€¢ Stress testing and scenario analysis\",\n",
    "    \"â€¢ Ready for immediate LLM consumption\"\n",
    "]\n",
    "\n",
    "for item in final_summary:\n",
    "    print(f\"  {item}\")\n",
    "\n",
    "print(f\"\\nğŸš€ Analysis completed at: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "689faae3",
   "metadata": {},
   "source": [
    "## ğŸ¯ Conclusion\n",
    "\n",
    "The **Analyzer** class provides a comprehensive solution for generating portfolio analysis optimized for Vision-Capable LLMs:\n",
    "\n",
    "### ğŸ† Key Achievements:\n",
    "- **150+ Metrics**: Complete performance, risk, and allocation analysis\n",
    "- **Portfolio Greeks**: Professional-grade sensitivity analysis\n",
    "- **10 Visualizations**: High-resolution charts as base64 images\n",
    "- **Multiple Formats**: Flexible export options for different use cases\n",
    "- **LLM-Ready**: Structured JSON format optimized for AI consumption\n",
    "\n",
    "### ğŸ”® Ready for AI Integration:\n",
    "The Analyzer generates everything needed for a Vision-Capable LLM to:\n",
    "- Analyze portfolio performance and risk metrics\n",
    "- Interpret visual charts and patterns\n",
    "- Provide investment recommendations\n",
    "- Identify optimization opportunities\n",
    "- Generate comprehensive reports\n",
    "\n",
    "This represents a complete bridge between quantitative portfolio analysis and AI-powered insights! ğŸš€"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
