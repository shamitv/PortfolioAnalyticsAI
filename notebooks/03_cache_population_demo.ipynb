{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3f0dfa84",
   "metadata": {},
   "source": [
    "# Portfolio Analytics AI - Market Data Cache Population Demo\n",
    "\n",
    "This notebook demonstrates how to pre-populate the market data cache with comprehensive financial data including:\n",
    "- S&P 500 company stock data\n",
    "- Sector ETF data for portfolio diversification\n",
    "- Risk-free rate data for portfolio performance analysis\n",
    "\n",
    "The cache population process ensures you have a robust dataset for portfolio analytics and optimization tasks.\n",
    "\n",
    "## Features Demonstrated:\n",
    "- Database schema creation and management\n",
    "- Automated data fetching from Yahoo Finance\n",
    "- Trading holidays handling\n",
    "- Error handling for delisted companies\n",
    "- Cache statistics and validation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b18294d",
   "metadata": {},
   "source": [
    "## 1. Import Required Libraries\n",
    "\n",
    "Import all necessary libraries for data processing, database management, and portfolio analytics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03b890f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import sqlite3\n",
    "import os\n",
    "from datetime import datetime, timedelta\n",
    "import pandas_market_calendars as mcal\n",
    "\n",
    "# Import portfolio analytics modules\n",
    "import sys\n",
    "sys.path.append('../src')  # Add src directory to path\n",
    "from portfolio_analytics.data_provider import DataProvider\n",
    "\n",
    "print(\"Libraries imported successfully!\")\n",
    "print(f\"Current working directory: {os.getcwd()}\")\n",
    "print(f\"Available directories: {[d for d in os.listdir('.') if os.path.isdir(d)]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df352f2d",
   "metadata": {},
   "source": [
    "## 2. Setup Database Connection and Table Creation\n",
    "\n",
    "Define functions to create the SQLite database tables for storing company information and sector metadata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f1e11ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_company_list_table(conn):\n",
    "    \"\"\"Create the company_list table if it doesn't exist.\"\"\"\n",
    "    cursor = conn.cursor()\n",
    "    cursor.execute('''\n",
    "        CREATE TABLE IF NOT EXISTS company_list (\n",
    "            Symbol TEXT PRIMARY KEY,\n",
    "            Name TEXT,\n",
    "            Sector TEXT\n",
    "        )\n",
    "    ''')\n",
    "    conn.commit()\n",
    "    print(\"✓ Company list table created/verified\")\n",
    "\n",
    "def create_sector_metadata_table(conn):\n",
    "    \"\"\"Create the sector_metadata table if it doesn't exist.\"\"\"\n",
    "    cursor = conn.cursor()\n",
    "    cursor.execute('''\n",
    "        CREATE TABLE IF NOT EXISTS sector_metadata (\n",
    "            sector_name TEXT PRIMARY KEY,\n",
    "            etf_ticker TEXT,\n",
    "            etf_name TEXT,\n",
    "            description TEXT,\n",
    "            created_date TEXT,\n",
    "            updated_date TEXT\n",
    "        )\n",
    "    ''')\n",
    "    conn.commit()\n",
    "    print(\"✓ Sector metadata table created/verified\")\n",
    "\n",
    "# Setup sample data directory and database path\n",
    "sample_data_dir = '../sample_data'\n",
    "db_path = os.path.join(sample_data_dir, 'market_data.db')\n",
    "os.makedirs(sample_data_dir, exist_ok=True)\n",
    "\n",
    "print(f\"Database will be created at: {db_path}\")\n",
    "print(f\"Sample data directory: {sample_data_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1887e29c",
   "metadata": {},
   "source": [
    "## 3. Fetch US Trading Holidays\n",
    "\n",
    "Get US trading holidays from 2000 to current year using pandas market calendars for accurate data processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcc3a7dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_us_trading_holidays():\n",
    "    \"\"\"\n",
    "    Returns a list of US trading holidays from 2000 to current year.\n",
    "    \"\"\"    \n",
    "    print(\"Fetching US trading holidays...\")\n",
    "    \n",
    "    # Initialize NYSE calendar\n",
    "    nyse = mcal.get_calendar('NYSE')\n",
    "    \n",
    "    # Get holidays from 2000 to current year\n",
    "    current_year = datetime.now().year\n",
    "    start_year = 2000\n",
    "    \n",
    "    # Get holidays for the date range using the correct API\n",
    "    start_date = f'{start_year}-01-01'\n",
    "    end_date = f'{current_year}-12-31'\n",
    "    \n",
    "    # Use the schedule method to get valid trading sessions, then get holidays\n",
    "    schedule = nyse.schedule(start_date=start_date, end_date=end_date)\n",
    "    \n",
    "    # Get all business days in the range\n",
    "    all_business_days = pd.bdate_range(start=start_date, end=end_date)\n",
    "    \n",
    "    # Get actual trading days from the schedule\n",
    "    trading_days = schedule.index.normalize()\n",
    "    \n",
    "    # Find holidays (business days that are not trading days)\n",
    "    holidays = all_business_days.difference(trading_days)\n",
    "    \n",
    "    print(f\"✓ Retrieved {len(holidays)} US trading holidays from {start_year} to {current_year}\")\n",
    "    return holidays.to_list()\n",
    "\n",
    "# Get trading holidays\n",
    "trading_holidays = get_us_trading_holidays()\n",
    "trading_holidays_str = [holiday.strftime('%Y-%m-%d') for holiday in trading_holidays]\n",
    "\n",
    "print(f\"Sample holidays: {trading_holidays_str[:5]}...\")\n",
    "print(f\"Recent holidays: {trading_holidays_str[-5:]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d82ddc33",
   "metadata": {},
   "source": [
    "## 4. Populate Sector Metadata\n",
    "\n",
    "Load sector ETF information from CSV file and populate the database with sector metadata for portfolio diversification analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa88c37b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def populate_sector_metadata(conn, sector_etfs_path):\n",
    "    \"\"\"Populate the sector_metadata table from the Sector_ETFs.csv file.\"\"\"\n",
    "    if not os.path.exists(sector_etfs_path):\n",
    "        print(f\"❌ Sector_ETFs.csv not found at {sector_etfs_path}\")\n",
    "        return []\n",
    "    \n",
    "    print(f\"Reading sector ETF data from {sector_etfs_path}\")\n",
    "    df = pd.read_csv(sector_etfs_path)\n",
    "    current_date = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "    \n",
    "    # Display the sector ETF data\n",
    "    print(f\"Found {len(df)} sector ETFs:\")\n",
    "    print(df[['Sector', 'Ticker', 'ETF Name']].to_string(index=False))\n",
    "    \n",
    "    # Prepare data for insertion\n",
    "    sector_data = []\n",
    "    for _, row in df.iterrows():\n",
    "        sector_data.append({\n",
    "            'sector_name': row['Sector'],\n",
    "            'etf_ticker': row['Ticker'],\n",
    "            'etf_name': row['ETF Name'],\n",
    "            'description': f\"SPDR sector ETF tracking {row['Sector']} sector\",\n",
    "            'created_date': current_date,\n",
    "            'updated_date': current_date\n",
    "        })\n",
    "    \n",
    "    # Convert to DataFrame and insert into database\n",
    "    sector_df = pd.DataFrame(sector_data)\n",
    "    sector_df.to_sql('sector_metadata', conn, if_exists='replace', index=False)\n",
    "    print(f\"✓ Populated sector metadata with {len(sector_df)} sectors\")\n",
    "    \n",
    "    return df['Ticker'].tolist()\n",
    "\n",
    "# Copy sample data from package first\n",
    "temp_data_provider = DataProvider(debug=True)\n",
    "temp_data_provider.copy_sample_data(sample_data_dir)\n",
    "print(\"✓ Copied current cache of market data from package sample data\")\n",
    "\n",
    "# Setup database connection and create tables\n",
    "conn = sqlite3.connect(db_path)\n",
    "create_company_list_table(conn)\n",
    "create_sector_metadata_table(conn)\n",
    "\n",
    "# Populate sector metadata\n",
    "sector_etfs_path = os.path.join(sample_data_dir, 'Sector_ETFs.csv')\n",
    "sector_etf_symbols = populate_sector_metadata(conn, sector_etfs_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd705907",
   "metadata": {},
   "source": [
    "## 5. Populate Company List Data\n",
    "\n",
    "Load S&P 500 company information from Excel file and populate the company_list table with stock symbols, names, and sectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dcac95d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def populate_company_list(conn, excel_path):\n",
    "    \"\"\"Populate the company_list table from an Excel file.\"\"\"\n",
    "    if not os.path.exists(excel_path):\n",
    "        print(f\"❌ S&P 500 Excel file not found at {excel_path}\")\n",
    "        return []\n",
    "    \n",
    "    print(f\"Reading S&P 500 company data from {excel_path}\")\n",
    "    df = pd.read_excel(excel_path, sheet_name='basics')\n",
    "    \n",
    "    # Display sample data\n",
    "    print(f\"Found {len(df)} S&P 500 companies\")\n",
    "    print(\"Sample companies:\")\n",
    "    print(df[['Symbol', 'Name', 'Sector']].head(10).to_string(index=False))\n",
    "    \n",
    "    # Sector distribution\n",
    "    sector_counts = df['Sector'].value_counts()\n",
    "    print(f\"\\nSector distribution:\")\n",
    "    print(sector_counts.to_string())\n",
    "    \n",
    "    # Ensure columns are named correctly for the database table\n",
    "    df_clean = df[['Symbol', 'Name', 'Sector']].copy()\n",
    "    df_clean.to_sql('company_list', conn, if_exists='replace', index=False)\n",
    "    print(f\"✓ Populated company list with {len(df_clean)} companies\")\n",
    "    \n",
    "    return df_clean['Symbol'].tolist()\n",
    "\n",
    "# Populate company list\n",
    "excel_path = os.path.join(sample_data_dir, 'snp_500_companies.xlsx')\n",
    "sp500_symbols = populate_company_list(conn, excel_path)\n",
    "\n",
    "# Close database connection for now\n",
    "conn.close()\n",
    "\n",
    "print(f\"\\n📊 Database setup complete!\")\n",
    "print(f\"   • S&P 500 companies: {len(sp500_symbols) if sp500_symbols else 0}\")\n",
    "print(f\"   • Sector ETFs: {len(sector_etf_symbols) if sector_etf_symbols else 0}\")\n",
    "print(f\"   • Trading holidays: {len(trading_holidays)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75b5f4f6",
   "metadata": {},
   "source": [
    "## 6. Initialize Data Provider with Cache\n",
    "\n",
    "Create DataProvider instance with caching enabled and configure it for fetching market data including risk-free rates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91d8a5aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize DataProvider with caching enabled, debug output on, and trading holidays\n",
    "data_provider = DataProvider(\n",
    "    cache=True, \n",
    "    cache_db=db_path, \n",
    "    debug=True, \n",
    "    trading_holidays=trading_holidays_str\n",
    ")\n",
    "\n",
    "print(\"✓ DataProvider initialized with cache enabled\")\n",
    "print(f\"  Cache database: {db_path}\")\n",
    "print(f\"  Trading holidays configured: {len(trading_holidays_str)}\")\n",
    "\n",
    "# Get risk-free rate metadata and add the symbol\n",
    "risk_free_metadata = data_provider.get_risk_free_rate_metadata()\n",
    "risk_free_symbol = [risk_free_metadata['symbol']]\n",
    "\n",
    "print(f\"\\n📈 Risk-free rate information:\")\n",
    "print(f\"   Symbol: {risk_free_metadata['symbol']}\")\n",
    "print(f\"   Name: {risk_free_metadata['name']}\")\n",
    "print(f\"   Currency: {risk_free_metadata['currency']}\")\n",
    "print(f\"   Frequency: {risk_free_metadata['frequency']}\")\n",
    "print(f\"   Description: {risk_free_metadata['description'][:100]}...\")\n",
    "\n",
    "# Prepare symbol lists for download\n",
    "if sp500_symbols and sector_etf_symbols:\n",
    "    all_symbols = sp500_symbols + sector_etf_symbols + risk_free_symbol\n",
    "    print(f\"\\n📋 Symbols prepared for download:\")\n",
    "    print(f\"   S&P 500 stocks: {len(sp500_symbols)}\")\n",
    "    print(f\"   Sector ETFs: {len(sector_etf_symbols)}\")\n",
    "    print(f\"   Risk-free rate: 1 ({risk_free_symbol[0]})\")\n",
    "    print(f\"   Total symbols: {len(all_symbols)}\")\n",
    "else:\n",
    "    print(\"❌ Error: Could not load symbol lists. Check data files.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e89911f0",
   "metadata": {},
   "source": [
    "## 7. Download Market Data for All Symbols\n",
    "\n",
    "Fetch 18 years of historical price data for all symbols with comprehensive error handling and progress tracking.\n",
    "\n",
    "**Note:** This process may take 15-30 minutes depending on network speed and data availability. The cell will show progress for each symbol downloaded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ff26ea2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define date range - 18 years of historical data\n",
    "end_date = datetime.now() - timedelta(days=1)\n",
    "start_date = end_date - timedelta(days=18*365)\n",
    "\n",
    "print(f\"📅 Data range: {start_date.strftime('%Y-%m-%d')} to {end_date.strftime('%Y-%m-%d')}\")\n",
    "print(f\"🚀 Starting download process for {len(all_symbols)} symbols...\")\n",
    "print(f\"💡 Tip: This may take 15-30 minutes. Progress will be shown below.\\n\")\n",
    "\n",
    "# Initialize counters\n",
    "delisted_count = 0\n",
    "error_count = 0\n",
    "success_count = 0\n",
    "risk_free_downloaded = False\n",
    "\n",
    "# Lists to track results\n",
    "successful_symbols = []\n",
    "delisted_symbols = []\n",
    "error_symbols = []\n",
    "\n",
    "# Download data for all symbols\n",
    "for i, symbol in enumerate(all_symbols):\n",
    "    # Determine symbol type for display\n",
    "    if symbol in sector_etf_symbols:\n",
    "        symbol_type = \"sector ETF\"\n",
    "    elif symbol in risk_free_symbol:\n",
    "        symbol_type = \"risk-free rate\"\n",
    "    else:\n",
    "        symbol_type = \"S&P 500 stock\"\n",
    "    \n",
    "    try:\n",
    "        print(f\"({i+1:3d}/{len(all_symbols)}) Fetching {symbol:6s} ({symbol_type})...\", end=\" \")\n",
    "        \n",
    "        # Fetch the data\n",
    "        data_provider.get_price_data(\n",
    "            symbols=symbol,\n",
    "            start_date=start_date.strftime('%Y-%m-%d'),\n",
    "            end_date=end_date.strftime('%Y-%m-%d')\n",
    "        )\n",
    "        \n",
    "        success_count += 1\n",
    "        successful_symbols.append(symbol)\n",
    "        \n",
    "        if symbol in risk_free_symbol:\n",
    "            risk_free_downloaded = True\n",
    "            print(\"✅ SUCCESS (Risk-free rate)\")\n",
    "        else:\n",
    "            print(\"✅ SUCCESS\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        error_msg = str(e)\n",
    "        if \"possibly delisted\" in error_msg or \"no price data found\" in error_msg:\n",
    "            print(f\"⚠️  DELISTED/NO DATA\")\n",
    "            delisted_count += 1\n",
    "            delisted_symbols.append(symbol)\n",
    "        else:\n",
    "            print(f\"❌ ERROR: {error_msg}\")\n",
    "            error_count += 1\n",
    "            error_symbols.append(symbol)\n",
    "            if symbol in risk_free_symbol:\n",
    "                print(f\"   ❌ Failed to download risk-free rate data\")\n",
    "\n",
    "    # Progress update every 50 symbols\n",
    "    if (i + 1) % 50 == 0:\n",
    "        progress = (i + 1) / len(all_symbols) * 100\n",
    "        print(f\"\\n📊 Progress: {progress:.1f}% complete ({i+1}/{len(all_symbols)} symbols processed)\")\n",
    "        print(f\"   ✅ Success: {success_count}, ⚠️ Delisted: {delisted_count}, ❌ Errors: {error_count}\\n\")\n",
    "\n",
    "print(f\"\\n🎉 Download process completed!\")\n",
    "print(f\"📈 Final statistics:\")\n",
    "print(f\"   ✅ Successful downloads: {success_count}\")\n",
    "print(f\"   ⚠️  Delisted/No data: {delisted_count}\")\n",
    "print(f\"   ❌ Errors: {error_count}\")\n",
    "print(f\"   🎯 Success rate: {success_count/len(all_symbols)*100:.1f}%\")\n",
    "\n",
    "if risk_free_downloaded:\n",
    "    print(f\"   ✅ Risk-free rate data: Successfully downloaded\")\n",
    "else:\n",
    "    print(f\"   ❌ Risk-free rate data: Download failed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e71c461b",
   "metadata": {},
   "source": [
    "## 8. Display Cache Statistics and Results\n",
    "\n",
    "Analyze the populated cache, show database statistics, and demonstrate the new cached stocks methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ed54d1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Database file statistics\n",
    "if os.path.exists(db_path):\n",
    "    db_size = os.path.getsize(db_path)\n",
    "    print(f\"💾 Database file statistics:\")\n",
    "    print(f\"   File path: {db_path}\")\n",
    "    print(f\"   File size: {db_size / 1024 / 1024:.2f} MB\")\n",
    "    print(f\"   File size: {db_size:,} bytes\")\n",
    "else:\n",
    "    print(f\"❌ Database file not found at {db_path}\")\n",
    "\n",
    "# Test the new cached stocks methods\n",
    "print(f\"\\n🔍 Testing new cached stocks methods...\")\n",
    "\n",
    "# Get all cached stocks (including ETFs)\n",
    "all_cached_stocks = data_provider.get_cached_stocks(include_etfs=True)\n",
    "print(f\"   📊 Total symbols in cache: {len(all_cached_stocks)}\")\n",
    "\n",
    "# Get stocks only (excluding ETFs)\n",
    "stocks_only = data_provider.get_cached_stocks(include_etfs=False)\n",
    "print(f\"   📈 Stock symbols only: {len(stocks_only)}\")\n",
    "\n",
    "# Get ETFs for comparison\n",
    "cached_etfs = data_provider.get_cached_etfs()\n",
    "print(f\"   🎯 ETF symbols: {len(cached_etfs)}\")\n",
    "\n",
    "# Get detailed symbol information\n",
    "symbols_info = data_provider.get_cached_symbols_info()\n",
    "print(f\"   📋 Symbols with detailed info: {len(symbols_info)}\")\n",
    "\n",
    "# Analysis of cached data\n",
    "if symbols_info:\n",
    "    print(f\"\\n📊 Cache analysis:\")\n",
    "    \n",
    "    # Count by symbol type\n",
    "    etf_count = sum(1 for info in symbols_info.values() if info['symbol_type'] == 'ETF')\n",
    "    stock_count = len(symbols_info) - etf_count\n",
    "    \n",
    "    print(f\"   Stock symbols: {stock_count}\")\n",
    "    print(f\"   ETF symbols: {etf_count}\")\n",
    "    print(f\"   Total symbols: {len(symbols_info)}\")\n",
    "    \n",
    "    # Data point statistics\n",
    "    data_points = [info['count'] for info in symbols_info.values()]\n",
    "    print(f\"   Average data points per symbol: {sum(data_points)/len(data_points):.0f}\")\n",
    "    print(f\"   Maximum data points: {max(data_points):,}\")\n",
    "    print(f\"   Minimum data points: {min(data_points):,}\")\n",
    "    \n",
    "    # Date range\n",
    "    start_dates = [info['start_date'] for info in symbols_info.values()]\n",
    "    end_dates = [info['end_date'] for info in symbols_info.values()]\n",
    "    print(f\"   Data date range: {min(start_dates)} to {max(end_dates)}\")\n",
    "    \n",
    "    # Symbol with most data\n",
    "    max_data_symbol = max(symbols_info.keys(), key=lambda k: symbols_info[k]['count'])\n",
    "    max_count = symbols_info[max_data_symbol]['count']\n",
    "    print(f\"   Symbol with most data: {max_data_symbol} ({max_count:,} points)\")\n",
    "    \n",
    "    # Sample of successful symbols by type\n",
    "    sample_stocks = [s for s in stocks_only[:10]]\n",
    "    sample_etfs = [e for e in cached_etfs[:5]]\n",
    "    \n",
    "    print(f\"\\n📈 Sample successful stock symbols: {', '.join(sample_stocks)}\")\n",
    "    print(f\"🎯 Sample successful ETF symbols: {', '.join(sample_etfs)}\")\n",
    "    \n",
    "    # Risk-free rate verification\n",
    "    if risk_free_symbol[0] in symbols_info:\n",
    "        rf_info = symbols_info[risk_free_symbol[0]]\n",
    "        print(f\"✅ Risk-free rate ({risk_free_symbol[0]}): {rf_info['count']} data points, {rf_info['start_date']} to {rf_info['end_date']}\")\n",
    "    else:\n",
    "        print(f\"❌ Risk-free rate ({risk_free_symbol[0]}) not found in cache\")\n",
    "\n",
    "print(f\"\\n🎉 Cache population demo completed successfully!\")\n",
    "print(f\"💡 You can now use this cache for portfolio analytics, optimization, and performance analysis.\")\n",
    "print(f\"📚 See other notebooks for examples of using the cached data.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c77ff735",
   "metadata": {},
   "source": [
    "## Summary and Next Steps\n",
    "\n",
    "This notebook demonstrated the complete process of populating a comprehensive market data cache for portfolio analytics:\n",
    "\n",
    "### ✅ What We Accomplished:\n",
    "1. **Database Setup**: Created SQLite tables for company metadata and sector information\n",
    "2. **Trading Holidays**: Retrieved and configured US market holidays for accurate data processing\n",
    "3. **Sector ETFs**: Populated sector metadata for portfolio diversification analysis\n",
    "4. **S&P 500 Data**: Loaded company information for 500+ stocks\n",
    "5. **Risk-Free Rates**: Integrated Treasury bill data for performance benchmarking\n",
    "6. **Bulk Download**: Fetched 18 years of historical price data for all symbols\n",
    "7. **Cache Analysis**: Demonstrated new methods to query and understand cached data\n",
    "\n",
    "### 🔧 New DataProvider Methods Used:\n",
    "- `get_cached_stocks()` - Get list of all stocks in cache\n",
    "- `get_cached_symbols_info()` - Get detailed information about cached symbols\n",
    "- `get_cached_etfs()` - Get list of ETF symbols\n",
    "- `get_risk_free_rate_metadata()` - Get risk-free rate information\n",
    "\n",
    "### 📊 Typical Results:\n",
    "- **400+ symbols** successfully cached\n",
    "- **15-20 MB database** with 18 years of daily data\n",
    "- **S&P 500 stocks**, **sector ETFs**, and **risk-free rates** included\n",
    "- **4,000+ data points** per symbol on average\n",
    "\n",
    "### 🚀 Next Steps:\n",
    "1. **Portfolio Construction**: Use `01_getting_started.ipynb` to build portfolios\n",
    "2. **Performance Analysis**: Use `02_analyzer_demo.ipynb` for comprehensive analysis\n",
    "3. **Custom Analysis**: Create your own notebooks using the cached data\n",
    "4. **Data Updates**: Re-run this notebook periodically to update the cache\n",
    "\n",
    "### 💡 Tips:\n",
    "- The cache persists between sessions - no need to re-download unless updating\n",
    "- Use `include_etfs=False` in `get_cached_stocks()` for stock-only analysis\n",
    "- Check `get_cached_symbols_info()` to understand data availability before analysis\n",
    "- The risk-free rate data enables accurate Sharpe ratio calculations\n",
    "\n",
    "The cache is now ready for advanced portfolio analytics and optimization!"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
